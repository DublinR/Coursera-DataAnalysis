\documentclass[12pt]{article}

%opening
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}


\section{Data Analysis Week 7 Quiz}


\subsection*{Worklist}
\begin{enumerate}
\item What is LOESS? (Q1)
\item Quadratic and Cubic Splines (Q2)
\item Root mean squared error (Q2)
\item The `simpleboot' Package (Q3)
\item The `one.boot' function (Q3)
\item Error rates
\end{enumerate}
\newpage
\subsection*{Prepping Samsung data set}
\begin{framed}
\begin{verbatim}
varnames = names(samsungData) 
gsub("\\-",".",varnames) -> varnames
gsub("\\,",".",varnames) -> varnames
gsub("\\(|\\)","",varnames) -> varnames
gsub("\\()","",varnames)->varnames
gsub(" ","",varnames)->varnames 

\end{verbatim}
\end{framed}
Part 2 - put a unique code at the end of each variable name to ensure unique idenity.
\begin{framed}
\begin{verbatim}
for(i in 1:561)
{ 
  varnames[i] = paste(varnames[i],sprintf("%03d", i))
}

\end{verbatim}
\end{framed}


\begin{framed}
\begin{verbatim}
names(samsungData) = varnames
samsungData$activity 
   =as.factor(samsungData$activity)
sd.tr <- tree(activity~.,samsungData)
summary(sd.tr)

\end{verbatim}
\end{framed}

\newpage

%-------------------------------------------------------------------------%
\subsection*{LOESS Regression}

\begin{itemize}
\item LOESS and LOWESS (locally weighted scatterplot smoothing, and also known as locally weighted polynomial regression) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model.
\item A smooth curve through a set of data points obtained with this statistical technique is called a Loess Curve, particularly when each smoothed value is given by a weighted quadratic least squares regression over the span of values of the y-axis scattergram criterion variable. 
\end{itemize}

\begin{figure}[h!]
\centering
\includegraphics[width=0.50\linewidth]{./guthriespdMeans}
%\caption{}
\label{fig:guthriespdMeans}
\end{figure}
\newpage
\begin{figure}[h!]
\centering
\includegraphics[width=0.50\linewidth]{./guthrieOLSlineonly}
%\caption{}
\label{fig:guthrieOLSlineonly}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[width=0.55\linewidth]{./defaultloess}
\caption{}
\label{fig:defaultloess}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{./guthriespd3BASIC}
\caption{}
\label{fig:guthriespd3BASIC}
\end{figure}
\newpage

How to determine the \textit{\textbf{span}} of a model? 
\begin{itemize}
\item A higher span smooths out the fit more, 
\item A lower span captures more trends but introduces statistical noise if there is too little data. 
\end{itemize}

\newpage
%-------------------------------------------------------------------------%
\subsection*{Question 1}

When the span parameter increases in a \textit{\textbf{loess}} fit which of the following is true
(pick one)?

\begin{itemize}
\item The fit gets more smooth and the bias increases.
\end{itemize}


%-------------------------------------------------------------------------%
\subsection*{Useful \texttt{R} Commands}
\begin{itemize}
\item \texttt{ns()} - Generate a Basis Matrix for Natural Cubic Splines
\item \texttt{install\_github()} - Install a package from GitHub (using the \textit{\textbf{devtools}} package)
\end{itemize}
%-------------------------------------------------------------------------%
\newpage
\subsection*{Question 2}
Notes on Splines:
\begin{verbatim} http://www.math.ucla.edu/~baker/149.1.02w/handouts/dd_splines.pdf
\end{verbatim}
Define a data set according to the code

\begin{framed}
\begin{verbatim}
set.seed(53535)
xValues = seq(0,2*pi,length=100)
yValues = rnorm(100) + sin(xValues)
\end{verbatim}
\end{framed}

Fit linear models with the `\textbf{yValues}` as outcome and a natural cubic spline model
for the `\textbf{xValues}` as the covariates. Fit the model with degrees of freedom equal
to each integer between 1 and 10. 


\noindent For each model, calculate the root mean
squared error (RMSE) between the fitted values and the observed `\textbf{yValues}` (the
`\texttt{rmse()}` function in \texttt{R} may help). At what number of degrees of freedom is there
the most dramatic drop in the RMSE? Why does this make sense?

\begin{framed}
\begin{verbatim}
library(splines)
lmQ2.1  <- lm(yValues ~ ns(xValues, df=1))
lmQ2.2  <- lm(yValues ~ ns(xValues, df=2))
lmQ2.3  <- lm(yValues ~ ns(xValues, df=3))
lmQ2.4  <- lm(yValues ~ ns(xValues, df=4))
lmQ2.5  <- lm(yValues ~ ns(xValues, df=5))
lmQ2.6  <- lm(yValues ~ ns(xValues, df=6))
lmQ2.7  <- lm(yValues ~ ns(xValues, df=7))
lmQ2.8  <- lm(yValues ~ ns(xValues, df=8))
lmQ2.9  <- lm(yValues ~ ns(xValues, df=9))
lmQ2.10 <- lm(yValues ~ ns(xValues, df=10))

library("devtools")
install_github("medley", "mewo2") 
library(medley)
\end{verbatim}
\end{framed}
\newpage

\begin{framed}
\begin{verbatim}
all.rmse.s <- c(rmse(yValues, predict(lmQ2.1,data=yValues)),
                rmse(yValues, predict(lmQ2.2,data=yValues)),
                rmse(yValues, predict(lmQ2.3,data=yValues)),
                rmse(yValues, predict(lmQ2.4,data=yValues)),
                rmse(yValues, predict(lmQ2.5,data=yValues)),
                rmse(yValues, predict(lmQ2.6,data=yValues)),
                rmse(yValues, predict(lmQ2.7,data=yValues)),
                rmse(yValues, predict(lmQ2.8,data=yValues)),
                rmse(yValues, predict(lmQ2.9,data=yValues)),
                rmse(yValues, predict(lmQ2.10,data=yValues)))
\end{verbatim}
\end{framed}
The RMSE drops between df=2 and df=3. This is because the sinusoidal model has one inflection points - like a cubic function.

%-------------------------------------------------------------------------%
\newpage
\subsection*{Bootstrapping}
Bootstrapping is a method for assigning \textbf{measures of accuracy} to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using only very simple methods.

Bootstrapping is a statistical technique that falls under the broader heading of resampling. Bootstrapping can be used in the estimation of nearly any statistic. It involves a relatively simple procedure, but repeated so many times that bootstrap techniques are heavily dependent upon computer calculations.

A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. Bootstrap is also an appropriate way to control and check the stability of the results.


Bootstrapping can be a very useful tool in statistics and it is very easily implemented in \texttt{R}. Bootstrapping comes in handy when there is doubt that the usual distributional assumptions and asymptotic results are valid and accurate. Bootstrapping is a nonparametric method which lets us compute estimated standard errors, confidence intervals and hypothesis testing.

Generally bootstrapping follows the same basic steps:

\begin{itemize}
\item[1.] Resample a given data set a specified number of times
\item[2.] Calculate a specific statistic from each sample
\item[3.] Find the standard deviation of the distribution of that statistic
\end{itemize}

%-------------------------------------------------------------------------%
\newpage
\subsection*{Question 3}
Good Tutorial: \\ http://www.statmethods.net/advstats/bootstrapping.html\\
\textit{(simpleboot: Simple Bootstrap Routines)}\\ 

Load the `\textbf{simpleboot}` package (you may have to install it first) with the
following commands:

\begin{framed}
\begin{verbatim}
library(simpleboot) 
data(airquality)
attach(airquality)
\end{verbatim}
\end{framed}

Calculate the 75th percentile of the `Wind` variable. Then set the seed to
883833 and use the `\texttt{one.boot}` function with 1,000 replications to calculate the
bootstrap standard error of the 75th percentile of the `Wind` variable.

\begin{framed}
\begin{verbatim}
quantile(airquality$Wind, .75)[[1]] ## 11.5
set.seed(883833)
obQ3 <- one.boot(airquality$Wind, 
  function(x, inds) quantile(x, .75)[[1]], R=1000)
sd(obQ3$t) # 0.5965868
\end{verbatim}
\end{framed}

%**The 75th percentile is: 11.5 The bootstrap s.d. is: 0.5965868**

%-------------------------------------------------------------------------%
\newpage
\subsection*{Question 4}

Load the \textit{\textbf{Cars93}} data:

\begin{framed}
\begin{verbatim}
data(Cars93,package="MASS")
\end{verbatim}
\end{framed}

Set the seed to 7363 and calculate three trees using the `\texttt{tree()}` function on
bootstrapped samples (samples with replacement). Each tree should treat the
`DriveTrain` variable as the outcome and `Price` and `Type` as covariates.
Predict the value of the following data frame

\begin{framed}
\begin{verbatim}
newdata = data.frame(Type = "Large",Price = 20)
\end{verbatim}
\end{framed}

with each tree and report the majority vote winner along with the percentage of
votes among the three trees for that value.

\begin{verbatim}
set.seed(7363)
sample1 <- Cars93[sample(nrow(Cars93), replace=TRUE),]
sample2 <- Cars93[sample(nrow(Cars93), replace=TRUE),]
sample3 <- Cars93[sample(nrow(Cars93), replace=TRUE),]

tree1Q4 <- tree(DriveTrain ~ Price + Type, data=sample1)
tree2Q4 <- tree(DriveTrain ~ Price + Type, data=sample2)
tree3Q4 <- tree(DriveTrain ~ Price + Type, data=sample3)

p1 <- predict(tree1Q4, data=newdata, type="class")
p2 <- predict(tree2Q4, data=newdata, type="class")
p3 <- predict(tree3Q4, data=newdata, type="class")

table(p1)
table(p2)
table(p3)

\end{verbatim}

%Front Percent of Votes = 66%~~
%TFront Percent of Votes = 100%**
%-------------------------------------------------------------------------%
\subsection*{Question 5}
Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual tree.\\
\textbf{Notes on Random Forest :}
\begin{verbatim}
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
\end{verbatim}

Load the vowel.train and vowel.test data sets:

\begin{framed}
\begin{verbatim}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
\end{verbatim}
\end{framed}
\begin{itemize}
\item Set the variable `y` to be a factor variable in both the training and test set.
Then set the seed to 33833. \item Fit (1) a random forest predictor relating the
factor variable `y` to the remaining variables and (2) an `svm` predictor using
the `\texttt{svm()}` function in the `\textit{\textbf{e1071}}` package. \item What are the error rates for the
two approaches on the test data set? \item What is the error rate when the two methods
agree on a prediction?
\end{itemize}
%-----------------%
\begin{framed}
\begin{verbatim}
library(randomForest)
library(e1071)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)

setseed(33833)

rfQ5  <- randomForest(y ~ ., data=vowel.train)
svmQ5 <- svm(y ~ ., data=vowel.train)
\end{verbatim}
\end{framed}

%---------------------------------------------------------------------------%
\end{document}
\begin{itemize}
\item Test error random forest = 0.4199134 
\item Test error svm = 0.3874459 
\item Test error both agree = 0.2823129
\end{itemize}
